[ft_instance_hyperparameter]
max_batch_size=8 ; Use for allocate the buffer
max_seq_len=128 ; The sequence length of position embedding table, should move to model hyper-parameter
beam_width=1 ; beam width for beam search
top_k=0 ; k value for top k sampling
top_p=0.5 ; p value for top p sampling
temperature=1.0 ; Use for sampling
repetition_penalty=2.0 ; Use for sampling
len_penalty=0.0
beam_search_diversity_rate=0.0
data_type=fp16
enable_custom_all_reduce=0

tensor_para_size=1
pipeline_para_size=1

model_name=gptj_6B
model_dir=../models/j6b_ckpt/

[request]
request_batch_size=8 # determine by the request
request_output_len=32 # determine by the request

[gptj_6B]
head_num=16
size_per_head=256
vocab_size=50400
decoder_layers=28
rotary_embedding=64
start_id=50256
end_id=50256
inter_size=16384
num_tasks=2 ;optional
prompt_learning_type=2 ;optional --> 0: no prompt, 1: soft_prompt, 2: prefix_prompt, 3: p/prompt_tuning

;prompt learning example (soft prompt doesn't need it)
[gptj_6B_task_0] ; task_name_id = 0
task_name=task_0
prompt_length=5
;optional
[gptj_6B_task_1] ; task_name_id = 1
task_name=task_1
prompt_length=10
